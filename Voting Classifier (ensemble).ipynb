{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance metrics report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes a confusion matrix which is used to compute the below functions\n",
    "def cm_maker(y, ypred, n_classes):\n",
    "    \n",
    "    low = [1, 2, 3]\n",
    "    high = [5, 6, 7]\n",
    "    \n",
    "    cm = np.zeros((n_classes, n_classes))\n",
    "    \n",
    "    for i, j in zip(ypred, y):\n",
    "        \n",
    "        if i in low:\n",
    "            i = i - 1\n",
    "        if i in high:\n",
    "            i = i - 2\n",
    "            \n",
    "        if j in low:\n",
    "            j = j - 1\n",
    "        if j in high:\n",
    "            j = j - 2\n",
    "            \n",
    "        cm[int(i), int(j)] += 1\n",
    "        \n",
    "    return cm\n",
    "\n",
    "\n",
    "# Function computes precision score\n",
    "def preci(cm, c):\n",
    "    \n",
    "    if sum(cm[c,:]) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return cm[c,c]/sum(cm[c,:])\n",
    "\n",
    "\n",
    "# Function computes recall score\n",
    "def recall(cm, c):\n",
    "    \n",
    "    return cm[c,c]/sum(cm[:,c])\n",
    "\n",
    "\n",
    "# Function computes f1-score\n",
    "def f1(cm, c):\n",
    "    if (preci(cm,c) + recall(cm,c)) == 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return 2 * (preci(cm,c) * recall(cm,c)) / (preci(cm,c) + recall(cm,c))\n",
    "    \n",
    "\n",
    "# Function computes weighted f1-score\n",
    "def weighted_f1(cm, n_classes):\n",
    "    co_su=cm.sum(axis=0)\n",
    "    n=cm.sum()\n",
    "    \n",
    "    weighted_f1_sum = 0\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        if co_su[c] != 0:\n",
    "            weighted_f1_sum += f1(cm, c) * co_su[c] / n\n",
    "\n",
    "    return round(weighted_f1_sum, 3)\n",
    "        \n",
    "\n",
    "# Function computes macro f1-score\n",
    "def macro_f1(cm, n_classes):\n",
    "    \n",
    "    f1_sum = 0\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        f1_sum += f1(cm, i)\n",
    "    \n",
    "    return round(f1_sum / n_classes, 3)\n",
    "\n",
    "\n",
    "# Function to get accuracy\n",
    "def accuracy(test_y, ypred):\n",
    "    \n",
    "    # Count of times where true labels equal predictions\n",
    "    true_positives = 0\n",
    "    for i in range(len(test_y)):\n",
    "        if test_y[i] == ypred[i]:\n",
    "            true_positives += 1\n",
    "    \n",
    "    return true_positives / len(test_y)\n",
    "\n",
    "\n",
    "# Combines functions above for a coherent performance report\n",
    "def performance_report(test_y, ypred, n_classes):\n",
    "    \n",
    "    class_labels = [1, 2, 3, 5, 6, 7]\n",
    "    cm = cm_maker(test_y, ypred, n_classes)\n",
    "    \n",
    "    print('\\nConfusion matrix for prediction:\\n', cm)\n",
    "    print('\\n\\nAccuracy for prediction:\\n', accuracy(test_y, ypred))\n",
    "    \n",
    "    print('\\n\\nMetrics for classes')\n",
    "    print('_______________________________________________________________________________')\n",
    "    print('Class\\t|\\tPrecision\\t|\\tRecall\\t\\t|\\tF1 Score')\n",
    "    print('_______________________________________________________________________________')\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        \n",
    "        print('\\nClass', class_labels[i],'|\\t',round(preci(cm, i), 3),'\\t\\t|\\t',round(recall(cm, i), 3),'\\t\\t|\\t',round(f1(cm, i), 3))\n",
    "    \n",
    "    print('\\n\\nWeighted F1 score:\\n', weighted_f1(cm, n_classes))\n",
    "    print('\\nMacro F1 score:\\n', macro_f1(cm, n_classes))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis to tranform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of PCA using numpy\n",
    "def PCA_dim_reduction(train_file, test_file, n_components):\n",
    "    \n",
    "    # Read data\n",
    "    train = np.loadtxt(train_file, skiprows=1, delimiter=\",\")\n",
    "    test = np.loadtxt(test_file, skiprows=1, delimiter=\",\")\n",
    "\n",
    "    train_x = train[:,:-1]  \n",
    "    train_y = train[:,-1]\n",
    "\n",
    "    test_x = test[:,:-1]\n",
    "    test_y = test[:,-1]\n",
    "    \n",
    "    # Standardize data\n",
    "    mean = np.mean(train_x, axis=0)\n",
    "    std = np.std(train_x, axis=0)\n",
    "    \n",
    "    train_x = (train_x - mean) / std\n",
    "    test_x = (test_x - mean) / std\n",
    "    \n",
    "    # Get covariance matrix\n",
    "    covmat = np.cov(train_x.T)\n",
    "    \n",
    "    # Get eigenvalues\n",
    "    eigval, eigvec = np.linalg.eig(covmat)\n",
    "    \n",
    "    # Get explained variance (if needed)\n",
    "    exp_var = []\n",
    "    for i in range(len(eigval)):\n",
    "        exp_var.append(eigval[i]/np.sum(eigval))\n",
    "    exp_var = sorted(exp_var, reverse=True)\n",
    "\n",
    "    # Pair up eigenvalues and vectors, so the vectors can be ranked by values\n",
    "    eigenpairs = []\n",
    "    for i in range(len(eigval)):\n",
    "        eigenpairs.append((eigval[i]/np.sum(eigval), eigvec[:, i].T))\n",
    "    eigenpairs = sorted(eigenpairs, reverse=True)\n",
    "    \n",
    "    # Put in the vectors from most to least important in a list\n",
    "    sorted_vectors = []\n",
    "    for i in range(len(eigenpairs)):\n",
    "        sorted_vectors.append(eigenpairs[i][1])\n",
    "    \n",
    "    # Make new variables to store the transformed data (the n principle coponents)\n",
    "    new_train_x = np.zeros((len(train_x), n_components))\n",
    "    new_test_x = np.zeros((len(test_x), n_components))\n",
    "    \n",
    "    # Transform the data into n principle components on the new subspace\n",
    "    for i in range(n_components):\n",
    "        new_train_x[:,i] = train_x @ sorted_vectors[i]\n",
    "        new_test_x[:,i] = test_x @ sorted_vectors[i]\n",
    "    \n",
    "    # Store transformed data (pc's) in old variables\n",
    "    train_x = new_train_x\n",
    "    test_x = new_test_x\n",
    "    \n",
    "    # Return the transformed train and test data + class labels\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned Voting Classifier on Multi-Class PC(n=5) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix for prediction:\n",
      " [[21.  1.  4.  0.  1.  0.]\n",
      " [ 0. 21.  1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  3.  0.  1.]\n",
      " [ 0.  0.  0.  0.  2.  0.]\n",
      " [ 0.  0.  0.  0.  0.  8.]]\n",
      "\n",
      "\n",
      "Accuracy for prediction:\n",
      " 0.8461538461538461\n",
      "\n",
      "\n",
      "Metrics for classes\n",
      "_______________________________________________________________________________\n",
      "Class\t|\tPrecision\t|\tRecall\t\t|\tF1 Score\n",
      "_______________________________________________________________________________\n",
      "\n",
      "Class 1 |\t 0.778 \t\t|\t 1.0 \t\t|\t 0.875\n",
      "\n",
      "Class 2 |\t 0.913 \t\t|\t 0.913 \t\t|\t 0.913\n",
      "\n",
      "Class 3 |\t 0 \t\t|\t 0.0 \t\t|\t 0\n",
      "\n",
      "Class 5 |\t 0.6 \t\t|\t 0.75 \t\t|\t 0.667\n",
      "\n",
      "Class 6 |\t 1.0 \t\t|\t 0.667 \t\t|\t 0.8\n",
      "\n",
      "Class 7 |\t 1.0 \t\t|\t 0.889 \t\t|\t 0.941\n",
      "\n",
      "\n",
      "Weighted F1 score:\n",
      " 0.814\n",
      "\n",
      "Macro F1 score:\n",
      " 0.699\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = PCA_dim_reduction('df_train.csv', 'df_test.csv', 5)\n",
    "\n",
    "clf1 = QuadraticDiscriminantAnalysis()\n",
    "clf2 = DecisionTreeClassifier(max_depth=2, min_samples_split=9)\n",
    "clf3 = SVC(C=1000, gamma=0.01, probability=True)\n",
    "clf4 = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "base_models = [('QDA', clf1), ('DT', clf2), ('SVM', clf3), ('KNN', clf4)]\n",
    "\n",
    "meta_model = VotingClassifier(estimators=base_models, voting = 'soft', weights=[3, 3, 5, 3]) # Weight chosen by finetuning\n",
    "\n",
    "meta_model.fit(train_x, train_y)\n",
    "\n",
    "pred_y = meta_model.predict(test_x)\n",
    "\n",
    "performance_report(test_y, meta_model.predict(test_x), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned Voting Classifier on Binary (window/non-window) PC(n=5) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to read in the data, transform the data to the first 5 principal components,\n",
    "and then make the data binary by combining window classes (1) and non-window classes (2)\n",
    "in two seperate classes.\n",
    "\"\"\"\n",
    "def binary_classification_data(train_file, test_file):    \n",
    "    # Read in the top 5 Principal Components\n",
    "    train_x, train_y, test_x, test_y = PCA_dim_reduction(train_file, test_file, 5)\n",
    "\n",
    "    # Divide test and train data into features and class labels\n",
    "    window = [1,2,3]\n",
    "\n",
    "    for i in range(len(train_y)):\n",
    "        if train_y[i] in window:\n",
    "            train_y[i] = 1\n",
    "        else:\n",
    "            train_y[i] = 2\n",
    "\n",
    "    for i in range(len(test_y)):\n",
    "        if test_y[i] in window:\n",
    "            test_y[i] = 1\n",
    "        else:\n",
    "            test_y[i] = 2\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20160 candidates, totalling 100800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1812 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 5312 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 10212 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 16512 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done 24212 tasks      | elapsed:   48.0s\n",
      "[Parallel(n_jobs=-1)]: Done 33312 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 43812 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 55712 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 69012 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 83712 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 99812 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 100800 out of 100800 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix for prediction:\n",
      " [[47.  1.]\n",
      " [ 2. 15.]]\n",
      "\n",
      "\n",
      "Accuracy for prediction:\n",
      " 0.9538461538461539\n",
      "\n",
      "\n",
      "Metrics for classes\n",
      "_______________________________________________________________________________\n",
      "Class\t|\tPrecision\t|\tRecall\t\t|\tF1 Score\n",
      "_______________________________________________________________________________\n",
      "\n",
      "Class 1 |\t 0.979 \t\t|\t 0.959 \t\t|\t 0.969\n",
      "\n",
      "Class 2 |\t 0.882 \t\t|\t 0.938 \t\t|\t 0.909\n",
      "\n",
      "\n",
      "Weighted F1 score:\n",
      " 0.954\n",
      "\n",
      "Macro F1 score:\n",
      " 0.939\n"
     ]
    }
   ],
   "source": [
    "# Read in binary train and test data + labels and store in variables\n",
    "train_x, train_y, test_x, test_y = binary_classification_data('df_train.csv', 'df_test.csv')\n",
    "\n",
    "# Our base models/classifiers\n",
    "clf1 = QuadraticDiscriminantAnalysis()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = SVC(probability=True)  # Probability set to True to enable soft voting (prediction probability based voting)\n",
    "clf4 = KNeighborsClassifier()\n",
    "\n",
    "# Stored in a list with a label to access them\n",
    "base_models = [('QDA', clf1), ('DT', clf2), ('SVC', clf3), ('KNN', clf4)]\n",
    "\n",
    "# Framework for voting classifier with soft voting enabled and our vanilla estimators\n",
    "clf = VotingClassifier(estimators=base_models, voting='soft')\n",
    "\n",
    "# A dictionary containing the hyperparameter names and the values we wish to include in the combinations\n",
    "params = {'DT__max_depth':range(1,10),\n",
    "          'DT__min_samples_split':range(2,10),\n",
    "          'SVC__C':[1, 10, 100, 1000],\n",
    "          'SVC__gamma':[0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "          'KNN__n_neighbors':range(1,15)\n",
    "}\n",
    "\n",
    "# 5-fold grid-search with backend print\n",
    "grid = GridSearchCV(clf, params, cv=5, verbose=1, n_jobs=-1)\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "# Create a variable for the best estimator\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_model.fit(train_x, train_y)\n",
    "\n",
    "# Make the model's prediction on the test data\n",
    "pred_y = best_model.predict(test_x)\n",
    "\n",
    "# Print the performance of the grid-searched model\n",
    "performance_report(test_y, best_model.predict(test_x), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned Binary Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix for prediction:\n",
      " [[47.  1.]\n",
      " [ 2. 15.]]\n",
      "\n",
      "\n",
      "Accuracy for prediction:\n",
      " 0.9538461538461539\n",
      "\n",
      "\n",
      "Metrics for classes\n",
      "_______________________________________________________________________________\n",
      "Class\t|\tPrecision\t|\tRecall\t\t|\tF1 Score\n",
      "_______________________________________________________________________________\n",
      "\n",
      "Class 1 |\t 0.979 \t\t|\t 0.959 \t\t|\t 0.969\n",
      "\n",
      "Class 2 |\t 0.882 \t\t|\t 0.938 \t\t|\t 0.909\n",
      "\n",
      "\n",
      "Weighted F1 score:\n",
      " 0.954\n",
      "\n",
      "Macro F1 score:\n",
      " 0.939\n"
     ]
    }
   ],
   "source": [
    "# Read in binary window/non-window 5 component pca-data\n",
    "train_x, train_y, test_x, test_y = binary_classification_data('df_train.csv', 'df_test.csv')\n",
    "\n",
    "# Try model from search above, if none then use earlier model\n",
    "try:\n",
    "    clf = best_model \n",
    "except NameError:\n",
    "    clf = VotingClassifier(estimators=\n",
    "                            [('QDA', QuadraticDiscriminantAnalysis()),\n",
    "                             ('DT', DecisionTreeClassifier(max_depth=2, min_samples_split=8)),\n",
    "                             ('SVC', SVC(C=100, gamma=0.1, probability=True)),\n",
    "                             ('KNN', KNeighborsClassifier(n_neighbors=1))],\n",
    "                        voting='soft')\n",
    "\n",
    "# fit the model\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# get predictions from the voting classifier\n",
    "pred_y = clf.predict(test_x)\n",
    "\n",
    "# print performance metrics from the predictions on the test data\n",
    "performance_report(test_y, clf.predict(test_x), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for the best weights (range 1-5 for all weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4, 4, 3]] 0.8615384615384616 [[3, 4, 5, 3]]\n"
     ]
    }
   ],
   "source": [
    "clf1 = QuadraticDiscriminantAnalysis()\n",
    "clf2 = DecisionTreeClassifier(max_depth=2, min_samples_split=9)\n",
    "clf3 = SVC(C=1000, gamma=0.01, probability=True)\n",
    "clf4 = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "base_models = [('QDA', clf1), ('DT', clf2), ('SVM', clf3), ('KNN', clf4)]\n",
    "\n",
    "weight = []\n",
    "i, j, k, l = 1,1,1,1\n",
    "for i in range(1,6):\n",
    "    weight.append([i, j, k, l])\n",
    "    for j in range(1,6):\n",
    "        weight.append([i, j, k, l])\n",
    "        for k in range(1,6):\n",
    "            weight.append([i, j, k, l])\n",
    "            for l in range(1,6):\n",
    "                weight.append([i, j, k, l])\n",
    "\n",
    "highest_score = 0\n",
    "best_weight = [[0,0,0,0]]\n",
    "also_best_weights = []\n",
    "\n",
    "for x in range(10):\n",
    "    for i in weight:\n",
    "        train_x, train_y, test_x, test_y = PCA_dim_reduction('df_train.csv', 'df_test.csv', 5)\n",
    "\n",
    "        meta_model = VotingClassifier(estimators=base_models, voting = 'soft', weights = i)\n",
    "        meta_model.fit(train_x, train_y)\n",
    "\n",
    "        if meta_model.score(test_x, test_y) == highest_score:\n",
    "            also_best_weights.append(i)\n",
    "\n",
    "        if meta_model.score(test_x, test_y) > highest_score:\n",
    "            highest_score = meta_model.score(test_x, test_y)\n",
    "            best_weight[0] = i\n",
    "            also_best_weights = []\n",
    "\n",
    "print(best_weight, highest_score, also_best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch for finetuning hyperparameters in Voting Classifier\n",
    "### The grid search will try all given combinations of hyperparameters and evaluate the best model based on a given scoring metric (in this case accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 22680 candidates, totalling 113400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 340 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3040 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 7540 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 13840 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=-1)]: Done 21940 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 31840 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 43540 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 57040 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 72340 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 89440 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 108340 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 113400 out of 113400 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7846153846153846, VotingClassifier(estimators=[('QDA', QuadraticDiscriminantAnalysis()),\n",
      "                             ('DT',\n",
      "                              DecisionTreeClassifier(max_depth=7,\n",
      "                                                     min_samples_split=5)),\n",
      "                             ('SVC', SVC(C=1000, gamma=0.1, probability=True)),\n",
      "                             ('KNN', KNeighborsClassifier(n_neighbors=1))],\n",
      "                 voting='soft'))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Our base models/classifiers\n",
    "clf1 = QuadraticDiscriminantAnalysis()\n",
    "clf2 = DecisionTreeClassifier(max_depth=1)\n",
    "clf3 = SVC(probability=True)  # Probability set to True to enable soft voting (prediction probability based voting)\n",
    "clf4 = KNeighborsClassifier()\n",
    "\n",
    "# Stored in a list with a label to access them\n",
    "base_models = [('QDA', clf1), ('DT', clf2), ('SVC', clf3), ('KNN', clf4)]\n",
    "\n",
    "# Framework for voting classifier with soft voting enabled and our vanilla estimators\n",
    "clf = VotingClassifier(estimators=base_models, voting='soft')\n",
    "\n",
    "# A dictionary containing the hyperparameter names and the values we wish to include in the combinations\n",
    "params = {'DT__max_depth':range(1,10),\n",
    "          'DT__min_samples_split':range(1,10),\n",
    "          'SVC__C':[1, 10, 100, 1000],\n",
    "          'SVC__gamma':[0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "          'KNN__n_neighbors':range(1,15)\n",
    "}\n",
    "\n",
    "best_models = []\n",
    "grid = GridSearchCV(clf, params, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid.fit(train_x, train_y)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "best_model.fit(train_x, train_y)\n",
    "    \n",
    "best_models.append((best_model.score(test_x, test_y), grid.best_estimator_))\n",
    "\n",
    "print(best_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIGHEST SCORING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_weighting(base_models):\n",
    "    \n",
    "    weight = []\n",
    "    i, j, k, l = 1,1,1,1\n",
    "    for i in range(1,6):\n",
    "        weight.append([i, j, k, l])\n",
    "        for j in range(1,6):\n",
    "            weight.append([i, j, k, l])\n",
    "            for k in range(1,6):\n",
    "                weight.append([i, j, k, l])\n",
    "                for l in range(1,6):\n",
    "                    weight.append([i, j, k, l])\n",
    "                    \n",
    "    highest_score = 0\n",
    "    best_weight = [[0,0,0,0]]\n",
    "    also_best_weights = []\n",
    "\n",
    "    for x in range(5):\n",
    "        for i in weight:\n",
    "            train_x, train_y, test_x, test_y = PCA_dim_reduction('df_train.csv', 'df_test.csv', 5)\n",
    "\n",
    "            meta_model = VotingClassifier(estimators=base_models, voting = 'soft', weights = i)\n",
    "            meta_model.fit(train_x, train_y)\n",
    "\n",
    "            if meta_model.score(test_x, test_y) == highest_score:\n",
    "                also_best_weights.append(i)\n",
    "\n",
    "            if meta_model.score(test_x, test_y) > highest_score:\n",
    "                highest_score = meta_model.score(test_x, test_y)\n",
    "                best_weight[0] = i\n",
    "                also_best_weights = []\n",
    "                \n",
    "    return (base_models, best_weight, highest_score, also_best_weights)\n",
    "\n",
    "model1 = best_weighting([('QDA', QuadraticDiscriminantAnalysis()),\n",
    "                         ('DT', DecisionTreeClassifier(max_depth=2, min_samples_split=8)),\n",
    "                         ('SVC', SVC(C=1000, gamma=0.01, probability=True)),\n",
    "                         ('KNN', KNeighborsClassifier(n_neighbors=2))])\n",
    "\n",
    "model2 = best_weighting([('QDA', QuadraticDiscriminantAnalysis()),\n",
    "                         ('DT', DecisionTreeClassifier(max_depth=2, min_samples_split=9)),\n",
    "                         ('SVC', SVC(C=1000, gamma=0.01, probability=True)),\n",
    "                         ('KNN', KNeighborsClassifier(n_neighbors=2))])\n",
    "\n",
    "model3 = best_weighting([('QDA', QuadraticDiscriminantAnalysis()),\n",
    "                         ('DT', DecisionTreeClassifier(max_depth=2, min_samples_split=4)),\n",
    "                         ('SVC', SVC(C=1000, gamma=0.01, probability=True)),\n",
    "                         ('KNN', KNeighborsClassifier(n_neighbors=2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model weight evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MODEL 1:\n",
      "\n",
      "average accuracy for model 1 weight 1: 0.8361538461538452\n",
      "\n",
      "\n",
      "MODEL 2:\n",
      "\n",
      "average accuracy for model 2 weight 1: 0.8347692307692303\n",
      "average accuracy for model 2 weight 2: 0.8361538461538457\n",
      "average accuracy for model 2 weight 3: 0.8344615384615384\n",
      "average accuracy for model 2 weight 4: 0.8370769230769227\n",
      "average accuracy for model 2 weight 5: 0.8346153846153844\n",
      "average accuracy for model 2 weight 6: 0.8349230769230767\n",
      "\n",
      "\n",
      "MODEL 3:\n",
      "\n",
      "average accuracy for model 3 weight 1: 0.8347692307692303\n",
      "average accuracy for model 3 weight 2: 0.8361538461538457\n",
      "average accuracy for model 3 weight 3: 0.8344615384615384\n",
      "average accuracy for model 3 weight 4: 0.8370769230769227\n",
      "average accuracy for model 3 weight 5: 0.8346153846153844\n",
      "average accuracy for model 3 weight 6: 0.8349230769230767\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = PCA_dim_reduction('df_train.csv', 'df_test.csv', 5)\n",
    "\n",
    "def average_acc_eval(basemodels, weight, n):\n",
    "    \n",
    "    train_x, train_y, test_x, test_y = PCA_dim_reduction('df_train.csv', 'df_test.csv', 5)\n",
    "    \n",
    "    sums = 0\n",
    "    for i in range(n):\n",
    "        meta_model = VotingClassifier(estimators=base_models, voting = 'soft', weights=[3, 3, 4, 3])\n",
    "\n",
    "        meta_model.fit(train_x, train_y)\n",
    "\n",
    "        pred_y = meta_model.predict(test_x)\n",
    "\n",
    "        correct = 0\n",
    "        \n",
    "        for i in range(len(test_y)):\n",
    "            if pred_y[i] == test_y[i]:\n",
    "                correct+=1\n",
    "\n",
    "        acc = correct / len(test_y)\n",
    "\n",
    "        sums += acc\n",
    "\n",
    "    return sums/n\n",
    "\n",
    "# Model 1\n",
    "clf1 = QuadraticDiscriminantAnalysis()\n",
    "clf2 = DecisionTreeClassifier(max_depth=2, min_samples_split=7)\n",
    "clf3 = SVC(C=1000, gamma=0.01, probability=True)\n",
    "clf4 = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "base_models = [('QDA', clf1), ('DT', clf2), ('SVM', clf3), ('KNN', clf4)]\n",
    "\n",
    "weights_model1 = [[3, 3, 4, 3]]\n",
    "\n",
    "avg_model1 = []\n",
    "for weight in weights_model1:\n",
    "    avg_model1.append(average_acc_eval([('QDA', clf1), ('DT', clf2), ('SVM', clf3), ('KNN', clf4)], weight, 100))\n",
    "\n",
    "\n",
    "# Model 2\n",
    "clf1 = QuadraticDiscriminantAnalysis()\n",
    "clf2 = DecisionTreeClassifier(max_depth=2, min_samples_split=8)\n",
    "clf3 = SVC(C=1000, gamma=0.01, probability=True)\n",
    "clf4 = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "base_models = [('QDA', clf1), ('DT', clf2), ('SVM', clf3), ('KNN', clf4)]\n",
    "\n",
    "weights_model2 = [[1, 1, 2, 1], [3, 3, 5, 2], [4, 4, 5, 3], [3, 3, 5, 3], [2, 2, 4, 2], [2, 2, 5, 2]]\n",
    "\n",
    "avg_model2 = []\n",
    "for weight in weights_model2:\n",
    "    avg_model2.append(average_acc_eval([('QDA', clf1), ('DT', clf2), ('SVM', clf3), ('KNN', clf4)], weight, 100))\n",
    "\n",
    "\n",
    "\n",
    "# Model 3\n",
    "clf1 = QuadraticDiscriminantAnalysis()\n",
    "clf2 = DecisionTreeClassifier(max_depth=2, min_samples_split=9)\n",
    "clf3 = SVC(C=1000, gamma=0.01, probability=True)\n",
    "clf4 = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "base_models = [('QDA', clf1), ('DT', clf2), ('SVM', clf3), ('KNN', clf4)]\n",
    "\n",
    "weights_model3 = [[2, 2, 3, 2], [1, 1, 3, 1], [2, 2, 4, 2], [3, 3, 5, 3], [2, 3, 3, 2], [3, 3, 4, 3], [2, 2, 4, 2]]\n",
    "\n",
    "avg_model3 = []\n",
    "for weight in weights_model3:\n",
    "    avg_model3.append(average_acc_eval([('QDA', clf1), ('DT', clf2), ('SVM', clf3), ('KNN', clf4)], weight, 100))\n",
    "\n",
    "\n",
    "#model2=[avg_acc_m2_w1, avg_acc_m2_w2, avg_acc_m2_w3, avg_acc_m2_w4, avg_acc_m2_w5, avg_acc_m2_w6]\n",
    "#model3=[avg_acc_m3_w1, avg_acc_m3_w2, avg_acc_m3_w3, avg_acc_m3_w4, avg_acc_m3_w5, avg_acc_m3_w6, avg_acc_m3_w7, avg_acc_m3_w8]\n",
    "\n",
    "print(\"\\n\\nMODEL 1:\\n\")\n",
    "for i in range(len(avg_model1)):\n",
    "    print(f\"average accuracy for model 1 weight {i+1}: {avg_model1[i]}\")\n",
    "    \n",
    "print(\"\\n\\nMODEL 2:\\n\")\n",
    "for i in range(len(avg_model2)):\n",
    "    print(f\"average accuracy for model 2 weight {i+1}: {avg_model3[i]}\")\n",
    "print(\"\\n\\nMODEL 3:\\n\")\n",
    "for i in range(len(avg_model2)):\n",
    "    print(f\"average accuracy for model 3 weight {i+1}: {avg_model3[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
